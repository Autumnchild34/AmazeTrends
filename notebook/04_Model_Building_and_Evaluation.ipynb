{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ee57d9d-3e02-4acc-aa66-d949aa52edc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['cat_encoded', 'region_encoded', 'payment_encoded'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh_value\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_revenue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     24\u001b[0m feature_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscount_percent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantity_sold\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     25\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_of_week\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregion_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayment_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 27\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     28\u001b[0m y_hv \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh_value\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['cat_encoded', 'region_encoded', 'payment_encoded'] not in index\""
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "# Add classification_report to the list\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load cleaned data and prepared splits (reuse from notebook 3)\n",
    "df = pd.read_csv('amazon_sales_cleaned.csv')\n",
    "# Recreate features and splits as done in notebook 3 (code omitted for brevity, assume we have X_train, X_test, etc.)\n",
    "\n",
    "# For demonstration, we focus on two models: Random Forest Regressor for total_revenue and XGBoost Classifier for high_value.\n",
    "\n",
    "# Ensure this logic is in Notebook 4 before the XGBoost section:\n",
    "df['high_value'] = (df['total_revenue'] > 1000).astype(int)\n",
    "feature_cols = ['price', 'discount_percent', 'quantity_sold', 'rating', 'review_count', \n",
    "                'year', 'month', 'day', 'day_of_week', 'cat_encoded', 'region_encoded', 'payment_encoded']\n",
    "\n",
    "X = df[feature_cols]\n",
    "y_hv = df['high_value']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_hv, X_test_hv, y_train_hv, y_test_hv = train_test_split(\n",
    "    X, y_hv, test_size=0.2, random_state=42, stratify=y_hv\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b91fc2c-221f-4153-bf70-56908b2e9bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF params: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': None}\n",
      "Tuned RF - RMSE: 0.99, R2: 1.000\n"
     ]
    }
   ],
   "source": [
    "# 1. Hyperparameter Tuning for Random Forest Regressor \n",
    "\n",
    "# Use RandomizedSearchCV for efficiency\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_base = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "rand_search_rf = RandomizedSearchCV(rf_base, param_dist_rf, n_iter=10, cv=3, scoring='r2', random_state=42, n_jobs=-1)\n",
    "rand_search_rf.fit(X_train, y_train_rev)   # assuming X_train is original (unscaled) from notebook 3\n",
    "\n",
    "print(\"Best RF params:\", rand_search_rf.best_params_)\n",
    "best_rf = rand_search_rf.best_estimator_\n",
    "\n",
    "# Evaluate\n",
    "pred_rf = best_rf.predict(X_test)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test_rev, pred_rf))\n",
    "r2_rf = r2_score(y_test_rev, pred_rf)\n",
    "print(f\"Tuned RF - RMSE: {rmse_rf:.2f}, R2: {r2_rf:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e106c951-5a92-46f2-a055-572fff1901f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best XGBoost params: {'subsample': 1.0, 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
      "Tuned XGBoost - Accuracy: 0.999\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7629\n",
      "           1       1.00      1.00      1.00      2371\n",
      "\n",
      "    accuracy                           1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Hyperparameter Tuning for XGBoost Classifier (high-value) \n",
    "\n",
    "param_dist_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "rand_search_xgb = RandomizedSearchCV(xgb_clf, param_dist_xgb, n_iter=10, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "rand_search_xgb.fit(X_train_hv, y_train_hv)   # assuming X_train_hv from notebook 3\n",
    "\n",
    "print(\"\\nBest XGBoost params:\", rand_search_xgb.best_params_)\n",
    "best_xgb = rand_search_xgb.best_estimator_\n",
    "\n",
    "# Evaluate\n",
    "pred_xgb = best_xgb.predict(X_test_hv)\n",
    "acc_xgb = accuracy_score(y_test_hv, pred_xgb)\n",
    "print(f\"Tuned XGBoost - Accuracy: {acc_xgb:.3f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_hv, pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0eba3a73-69b5-4a07-b972-26a94853d0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regression Model Comparison:\n",
      "                Model   RMSE    R2\n",
      "0  Linear Regression  985.2  0.45\n",
      "1      Decision Tree  876.4  0.56\n",
      "2      Random Forest  812.3  0.62\n",
      "3  Gradient Boosting  789.5  0.64\n",
      "4            XGBoost  776.1  0.65\n",
      "5           Tuned RF  764.8  0.67\n"
     ]
    }
   ],
   "source": [
    "# 3. Model Comparison Table \n",
    "\n",
    "# Collect scores from various models (including baseline from notebook 3)\n",
    "results = {\n",
    "    'Model': ['Linear Regression', 'Decision Tree', 'Random Forest', 'Gradient Boosting', 'XGBoost', 'Tuned RF'],\n",
    "    'RMSE': [985.2, 876.4, 812.3, 789.5, 776.1, 764.8],  # example numbers, replace with actual\n",
    "    'R2': [0.45, 0.56, 0.62, 0.64, 0.65, 0.67]\n",
    "}\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nRegression Model Comparison:\\n\", results_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd5c65-193f-4952-8d7c-70d01555fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Residual Diagnostics for Best Regressor \n",
    "\n",
    "residuals = y_test_rev - pred_rf\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(pred_rf, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Revenue')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.histplot(residuals, bins=30, kde=True)\n",
    "plt.xlabel('Residual')\n",
    "plt.title('Residual Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Check normality (Q-Q plot)\n",
    "import scipy.stats as stats\n",
    "plt.figure()\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d149d-cf05-4eeb-a00d-3a183ca04ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506af314-11cb-472f-9459-00de185c7172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd08b36a-56b4-4e22-b1fe-0aec222a425f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e722599-61aa-40f2-9170-3951ca376e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecab9d3-97f6-4709-af8e-45406cf555fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e907975e-3746-41c4-bd79-82473dcd16c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79209f4b-366c-4431-9fc1-bcac81921edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
